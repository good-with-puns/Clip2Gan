{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zTzlX8UG0U-",
        "outputId": "66cba71b-fe55-484f-ae82-71d7b827de1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-akhrw1k7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-akhrw1k7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=7c2e9a98117d451cfc275fe6d790e869edd51ca7a62d3461ca5dd2dcc318161f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j5wwem37/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QlN2ybvW2mt",
        "outputId": "96b7f74e-d24c-477d-be73-bbd8fc079bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "import clip\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "ymB5eVciG4yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/img_align_celeba.zip'  # Adjust path as needed\n",
        "extract_path = '/content/data/'  # Path to extract files to\n",
        "\n",
        "# Create the extract directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f'Files extracted to {extract_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbeSmf-VWyXO",
        "outputId": "a24127f4-6cde-420d-d68b-f6cff2d601ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
        "clip_model.eval()\n",
        "\n",
        "# Load ProGAN from PyTorch Hub\n",
        "progan = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-256', pretrained=True, useGPU=torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVvz4Q1kG82S",
        "outputId": "9e1767af-0ee2-44db-83e8-b7e8afa00235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 55.7MiB/s]\n",
            "Downloading: \"https://github.com/facebookresearch/pytorch_GAN_zoo/zipball/hub\" to /root/.cache/torch/hub/hub.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/gan_zoo/PGAN/celebaHQ_s6_i80000-6196db68.pth\" to /root/.cache/torch/hub/checkpoints/celebaHQ_s6_i80000-6196db68.pth\n",
            "100%|██████████| 264M/264M [00:02<00:00, 115MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average network found !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "0Tigkx5N71Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=512, latent_dim=512):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(12):\n",
        "            layers.append(nn.Linear(input_dim, input_dim))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "        layers.append(nn.Linear(input_dim, latent_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "mJ72zwwYG-tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "iiJDU_FxxReV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # Define any transformations you want to apply to your images\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "# ])\n",
        "\n",
        "# # Use ImageFolder if you have a directory structure like:\n",
        "# # /content/data/train/class1\n",
        "# # /content/data/train/class2\n",
        "# # where each folder represents a class\n",
        "# dataset = datasets.ImageFolder(root='/content/data/img_align_celeba', transform=transform)\n",
        "\n",
        "# # Create the DataLoader\n",
        "# batch_size = 16\n",
        "# celeba_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Check the first batch\n",
        "# data_iter = iter(celeba_loader)\n",
        "# images, labels = data_iter.next()\n",
        "# print(images.shape)  # Check the shape of the images\n",
        "# print(labels.shape)  # Check the shape of the labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "KA1ZUDoqXCyQ",
        "outputId": "ee9c2484-52b0-4444-cd94-773303e30f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Couldn't find any class folder in /content/data/img_align_celeba.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-dec87aab0cdf>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# /content/data/train/class2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# where each folder represents a class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/data/img_align_celeba'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Create the DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in /content/data/img_align_celeba."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_network = MappingNetwork().to(DEVICE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "# ])\n",
        "# celeba_dataset = datasets.CelebA(root='./data/img_align_celeba', split='train', transform=transform, download=False)\n",
        "# celeba_loader = torch.utils.data.DataLoader(celeba_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "WGSndhIkHBkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class RawImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Directory with raw images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('jpg', 'jpeg', 'png'))]  # Filter image files\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "\n",
        "        # Open the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Ensure 3-channel RGB\n",
        "\n",
        "        # Apply transformations if defined\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Define transformations (optional)\n",
        "# Define any transformations you want to apply to your images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "\n",
        "# Initialize the dataset\n",
        "dataset = RawImageDataset(image_dir='/content/data/img_align_celeba', transform=transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 16\n",
        "celeba_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Verify the first batch of data\n",
        "# data_iter = iter(dataloader)\n",
        "# images = data_iter.next()\n",
        "# print(images.shape)  # Should print (batch_size, 3, 128, 128) if using the transformations above\n"
      ],
      "metadata": {
        "id": "KgBw7u2ua70V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "discriminator = nn.Sequential(\n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(128 * (IMAGE_SIZE // 4) ** 2, 1)\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "uwKFCwzlWBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code to check if there is a .pth file available for discriminator as well as mapping network if yes load the weights to it and use it in further part of the code\n",
        "\n",
        "import os\n",
        "\n",
        "# Check for discriminator .pth file\n",
        "discriminator_path = 'discriminator.pth'  # Replace with your actual path\n",
        "if os.path.exists(discriminator_path):\n",
        "    print(\"Discriminator weights found. Loading...\")\n",
        "    discriminator.load_state_dict(torch.load(discriminator_path, map_location=DEVICE))\n",
        "    discriminator.eval()  # Set to evaluation mode\n",
        "else:\n",
        "    print(\"Discriminator weights not found.\")\n",
        "\n",
        "# Check for mapping network .pth file\n",
        "mapping_network_path = 'mapping_network_progan.pth'  # Replace with your actual path\n",
        "if os.path.exists(mapping_network_path):\n",
        "    print(\"Mapping Network weights found. Loading...\")\n",
        "    mapping_network.load_state_dict(torch.load(mapping_network_path, map_location=DEVICE))\n",
        "    mapping_network.eval()  # Set to evaluation mode\n",
        "else:\n",
        "    print(\"Mapping Network weights not found.\")"
      ],
      "metadata": {
        "id": "jvvUf89PWKtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "663019bd-b8d5-439b-f83b-9f20ec5ce333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator weights found. Loading...\n",
            "Mapping Network weights found. Loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-c155e49d95d6>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  discriminator.load_state_dict(torch.load(discriminator_path, map_location=DEVICE))\n",
            "<ipython-input-12-c155e49d95d6>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mapping_network.load_state_dict(torch.load(mapping_network_path, map_location=DEVICE))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion_reconstruction = nn.MSELoss()\n",
        "criterion_identity = nn.CosineSimilarity(dim=1)\n",
        "criterion_adversarial = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define a simple discriminator for adversarial loss\n",
        "\n",
        "\n",
        "# Perceptual Loss Model (LPIPS)\n",
        "perceptual_model = models.alexnet(pretrained=True).features.to(DEVICE).eval()\n",
        "\n",
        "# Diversity Loss\n",
        "def diversity_loss(feature_1, feature_2, img_1, img_2):\n",
        "    # Flatten images to match feature-like structure\n",
        "    img_1_flat = img_1.view(img_1.size(0), -1)  # [B, C*H*W]\n",
        "    img_2_flat = img_2.view(img_2.size(0), -1)  # [B, C*H*W]\n",
        "\n",
        "    # Compute L1 norms\n",
        "    dist_features = torch.norm(feature_1 - feature_2, p=1, dim=1)  # [B]\n",
        "    dist_images = torch.norm(img_1_flat - img_2_flat, p=1, dim=1)  # [B]\n",
        "\n",
        "    # Compute mean diversity loss\n",
        "    return torch.mean(dist_features / (dist_images + 1e-8))  # Avoid division by zero\n",
        "\n",
        "\n",
        "# Optimizers\n",
        "optimizer = optim.Adam(mapping_network.parameters(), lr=LEARNING_RATE)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE)\n"
      ],
      "metadata": {
        "id": "FYDA8gCfwsG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "398ce302-08fa-4f78-c4d7-544162dbb867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 150MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    for images in celeba_loader:\n",
        "        images = images.to(DEVICE)  # Ensure images are on the correct device\n",
        "\n",
        "        # 1. Encode images with CLIP\n",
        "        with torch.no_grad():\n",
        "            clip_features = clip_model.encode_image(images).float().to(DEVICE)\n",
        "\n",
        "        # 2. Map CLIP features to ProGAN latent space\n",
        "        latent_vectors = mapping_network(clip_features).to(DEVICE)\n",
        "\n",
        "        # 3. Generate images using ProGAN\n",
        "        generated_images = progan.test(latent_vectors).to(DEVICE)  # Latent vectors are mapped directly to ProGAN\n",
        "\n",
        "        # 4. Compute Reconstruction Loss\n",
        "        loss_reconstruction = criterion_reconstruction(\n",
        "            F.interpolate(generated_images, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False),\n",
        "            images\n",
        "        )\n",
        "\n",
        "        # 5. Compute Perceptual Loss\n",
        "        with torch.no_grad():\n",
        "            features_real = perceptual_model(images)\n",
        "            features_fake = perceptual_model(F.interpolate(generated_images, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False))\n",
        "        loss_perceptual = torch.mean((features_real - features_fake) ** 2)\n",
        "        generated_images_resized = F.interpolate(generated_images, size=(224, 224), mode='bilinear', align_corners=False).to(DEVICE)\n",
        "\n",
        "        # 6. Compute Adversarial Loss\n",
        "        real_preds = discriminator(images)\n",
        "        fake_preds = discriminator(generated_images_resized.detach().detach())\n",
        "        loss_adv_real = criterion_adversarial(real_preds, torch.ones_like(real_preds))\n",
        "        loss_adv_fake = criterion_adversarial(fake_preds, torch.zeros_like(fake_preds))\n",
        "        loss_adversarial = loss_adv_real + loss_adv_fake\n",
        "\n",
        "        # 7. Compute Identity Loss\n",
        "        loss_identity = 1 - criterion_identity(clip_features, clip_model.encode_image(F.interpolate(generated_images, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False)).float())\n",
        "\n",
        "        # 8. Compute Diversity Loss\n",
        "        noise = torch.randn_like(clip_features).to(DEVICE)\n",
        "        perturbed_features = clip_features + noise\n",
        "        perturbed_features = perturbed_features.to(DEVICE)\n",
        "        perturbed_latents = mapping_network(perturbed_features)\n",
        "        diverse_images = progan.test(perturbed_latents).to(DEVICE)\n",
        "        loss_diversity = diversity_loss(\n",
        "            clip_features,\n",
        "            perturbed_features,\n",
        "            generated_images,\n",
        "            diverse_images\n",
        "        )\n",
        "\n",
        "        # 9. Compute Total Loss\n",
        "        total_loss = (\n",
        "            loss_reconstruction +\n",
        "            0.1 * loss_perceptual +\n",
        "            0.01 * loss_adversarial +\n",
        "            0.1 * loss_identity +\n",
        "            0.1 * loss_diversity\n",
        "        )\n",
        "        total_loss = total_loss.mean()\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        discriminator_optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "# Save the trained mapping network\n",
        "torch.save(mapping_network.state_dict(), \"mapping_network_newweights.pth\")\n",
        "\n"
      ],
      "metadata": {
        "id": "S-IDd4s_75Fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18effe3-06ba-4dad-d52b-514b893e7446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Loss: 1.3112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: also save the weights of discriminator as we are saving of the mapping network\n",
        "\n",
        "# Save the trained discriminator\n",
        "torch.save(discriminator.state_dict(), \"discriminator_newweights.pth\")"
      ],
      "metadata": {
        "id": "JCxOkdFd76TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import clip\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
        "clip_model.eval()\n",
        "\n",
        "# Load ProGAN from PyTorch Hub\n",
        "progan = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-256', pretrained=True, useGPU=torch.cuda.is_available())\n",
        "\n",
        "# Load the trained Mapping Network\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=512, latent_dim=512):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(12):\n",
        "            layers.append(nn.Linear(input_dim, input_dim))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "        layers.append(nn.Linear(input_dim, latent_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "mapping_network = MappingNetwork().to(DEVICE)\n",
        "mapping_network.load_state_dict(torch.load(\"/content/mapping_network_newweights.pth\", map_location=DEVICE))\n",
        "mapping_network.eval()\n",
        "\n",
        "# Function for Generating Images from Text\n",
        "def generate_image_from_text(text, output_path=\"generated_image_new.png\"):\n",
        "    # Encode text using CLIP\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(clip.tokenize([text]).to(DEVICE)).float()\n",
        "\n",
        "    # Map text features to ProGAN latent space\n",
        "    with torch.no_grad():\n",
        "        latent_vector = mapping_network(text_features)\n",
        "\n",
        "    # Generate image using ProGAN\n",
        "    with torch.no_grad():\n",
        "        generated_image = progan.test(latent_vector)\n",
        "\n",
        "    # Convert image to PIL format and save\n",
        "    generated_image = (generated_image[0].clamp(-1, 1) + 1) / 2  # Normalize to [0, 1]\n",
        "    generated_image = (generated_image * 255).permute(1, 2, 0).cpu().numpy().astype('uint8')\n",
        "    Image.fromarray(generated_image).save(output_path)\n",
        "    print(f\"Image saved at {output_path}\")\n",
        "\n",
        "# Example Usage\n",
        "text_description = \"woman\"\n",
        "generate_image_from_text(text_description)\n"
      ],
      "metadata": {
        "id": "MvBGE2lWXgjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f91a56d8-9708-458e-f0bb-62b68df5c8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorch_GAN_zoo_hub\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average network found !\n",
            "Image saved at generated_image_new.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-4fa9421a6f38>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  mapping_network.load_state_dict(torch.load(\"/content/mapping_network_newweights.pth\", map_location=DEVICE))\n"
          ]
        }
      ]
    }
  ]
}