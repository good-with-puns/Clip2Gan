{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zTzlX8UG0U-",
        "outputId": "56544055-9a12-495b-df15-47aa24beac70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-5f8dnqg3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-5f8dnqg3\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=ef489fb61d7487428fc7088f4aa3ffb469ffaeb28bc0fc9ecaacce1c06b9fe3a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-luvk3k22/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QlN2ybvW2mt",
        "outputId": "535d2ce7-6aba-4665-ff5b-3164874bdf07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "import clip\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "ymB5eVciG4yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/img_align_celeba.zip'  # Adjust path as needed\n",
        "extract_path = '/content/data/'  # Path to extract files to\n",
        "\n",
        "# Create the extract directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f'Files extracted to {extract_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbeSmf-VWyXO",
        "outputId": "c4c7d1eb-3911-40ae-d6ee-dda3791a1426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
        "clip_model.eval()\n",
        "\n",
        "# Load ProGAN from PyTorch Hub\n",
        "progan = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-256', pretrained=True, useGPU=torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVvz4Q1kG82S",
        "outputId": "2e5cae7b-a0e5-44cd-ddf0-e12c261c3146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 80.9MiB/s]\n",
            "Downloading: \"https://github.com/facebookresearch/pytorch_GAN_zoo/zipball/hub\" to /root/.cache/torch/hub/hub.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/gan_zoo/PGAN/celebaHQ_s6_i80000-6196db68.pth\" to /root/.cache/torch/hub/checkpoints/celebaHQ_s6_i80000-6196db68.pth\n",
            "100%|██████████| 264M/264M [00:01<00:00, 211MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average network found !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "0Tigkx5N71Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=512, latent_dim=512):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(12):\n",
        "            layers.append(nn.Linear(input_dim, input_dim))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "        layers.append(nn.Linear(input_dim, latent_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "mJ72zwwYG-tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "iiJDU_FxxReV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapping_network = MappingNetwork().to(DEVICE)\n"
      ],
      "metadata": {
        "id": "WGSndhIkHBkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class RawImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Directory with raw images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('jpg', 'jpeg', 'png'))]  # Filter image files\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_files[idx]\n",
        "        image_path = os.path.join(self.image_dir, image_name)\n",
        "\n",
        "        # Open the image\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Ensure 3-channel RGB\n",
        "\n",
        "        # Apply transformations if defined\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# Define transformations (optional)\n",
        "# Define any transformations you want to apply to your images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "\n",
        "# Initialize the dataset\n",
        "dataset = RawImageDataset(image_dir='/content/data/img_align_celeba', transform=transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 16\n",
        "celeba_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# # Verify the first batch of data\n",
        "# data_iter = iter(dataloader)\n",
        "# images = data_iter.next()\n",
        "# print(images.shape)  # Should print (batch_size, 3, 128, 128) if using the transformations above\n"
      ],
      "metadata": {
        "id": "KgBw7u2ua70V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(celeba_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlYzBzYGriES",
        "outputId": "9970f673-99bf-48ff-9d29-eded11d291ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12663"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: out of the 12663 batches, i want to train only on the first 5000 batches\n",
        "\n",
        "# ... (Your existing code)\n",
        "\n",
        "# Initialize the dataset\n",
        "dataset = RawImageDataset(image_dir='/content/data/img_align_celeba', transform=transform)\n",
        "\n",
        "# Create a DataLoader with limited number of batches\n",
        "num_batches_to_use = 5000\n",
        "subset_indices = list(range(min(num_batches_to_use * batch_size, len(dataset))))  # Calculate indices to include\n",
        "\n",
        "subset_dataset = torch.utils.data.Subset(dataset, subset_indices)\n",
        "\n",
        "celeba_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ... (Rest of your code)"
      ],
      "metadata": {
        "id": "qQJ0U1qetI7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(celeba_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZYVnXUytbtK",
        "outputId": "7e954c8e-45ea-4309-ea06-e2f633b64386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "discriminator = nn.Sequential(\n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "    nn.LeakyReLU(0.2),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(128 * (IMAGE_SIZE // 4) ** 2, 1)  # Single scalar output\n",
        ").to(DEVICE)"
      ],
      "metadata": {
        "id": "uwKFCwzlWBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write code to check if there is a .pth file available for discriminator as well as mapping network if yes load the weights to it and use it in further part of the code\n",
        "\n",
        "import os\n",
        "\n",
        "# Check for discriminator .pth file\n",
        "discriminator_path = 'discriminator.pth'  # Replace with your actual path\n",
        "if os.path.exists(discriminator_path):\n",
        "    print(\"Discriminator weights found. Loading...\")\n",
        "    discriminator.load_state_dict(torch.load(discriminator_path, map_location=DEVICE))\n",
        "    discriminator.eval()  # Set to evaluation mode\n",
        "else:\n",
        "    print(\"Discriminator weights not found.\")\n",
        "\n",
        "# Check for mapping network .pth file\n",
        "mapping_network_path = 'mapping_network_progan.pth'  # Replace with your actual path\n",
        "if os.path.exists(mapping_network_path):\n",
        "    print(\"Mapping Network weights found. Loading...\")\n",
        "    mapping_network.load_state_dict(torch.load(mapping_network_path, map_location=DEVICE))\n",
        "    mapping_network.eval()  # Set to evaluation mode\n",
        "else:\n",
        "    print(\"Mapping Network weights not found.\")"
      ],
      "metadata": {
        "id": "jvvUf89PWKtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a7d1c0-7d92-4605-93e6-d6085d7737eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator weights not found.\n",
            "Mapping Network weights not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(discriminator, real_data, fake_data):\n",
        "    batch_size = real_data.size(0)\n",
        "    alpha = torch.rand(batch_size, 1, 1, 1).to(DEVICE)\n",
        "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
        "    interpolates.requires_grad_(True)\n",
        "\n",
        "    disc_interpolates = discriminator(interpolates)\n",
        "    grad_outputs = torch.ones_like(disc_interpolates).to(DEVICE)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=disc_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=grad_outputs,\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "    return penalty"
      ],
      "metadata": {
        "id": "1EEJbgB8w3H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion_reconstruction = nn.MSELoss()\n",
        "criterion_identity = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "# Define a simple discriminator for adversarial loss\n",
        "\n",
        "\n",
        "# Perceptual Loss Model (LPIPS)\n",
        "perceptual_model = models.alexnet(pretrained=True).features.to(DEVICE).eval()\n",
        "\n",
        "# Diversity Loss\n",
        "def diversity_loss(feature_1, feature_2, img_1, img_2, scaling_factor=10.0):\n",
        "    img_1_flat = img_1.view(img_1.size(0), -1)\n",
        "    img_2_flat = img_2.view(img_2.size(0), -1)\n",
        "    dist_features = torch.norm(feature_1 - feature_2, p=1, dim=1)\n",
        "    dist_images = torch.norm(img_1_flat - img_2_flat, p=1, dim=1)\n",
        "    diversity_loss = torch.mean(dist_features / (dist_images + 1e-8))\n",
        "    return scaling_factor * diversity_loss  # Scaled for impact\n",
        "\n",
        "\n",
        "# Optimizers\n",
        "optimizer = optim.Adam(mapping_network.parameters(), lr=LEARNING_RATE)\n",
        "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE)\n"
      ],
      "metadata": {
        "id": "FYDA8gCfwsG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e524a888-7a2f-4167-b8de-db102b006241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:05<00:00, 43.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    for i, images in enumerate(celeba_loader):\n",
        "        images = images.to(DEVICE)\n",
        "\n",
        "        # Train Discriminator\n",
        "        for _ in range(5):\n",
        "            clip_features = clip_model.encode_image(images).float().to(DEVICE)\n",
        "            latent_vectors = mapping_network(clip_features).to(DEVICE)\n",
        "\n",
        "            # Noise injection during generation\n",
        "            noise = torch.randn_like(latent_vectors).to(DEVICE)\n",
        "            latent_vectors = latent_vectors + 0.05 * noise\n",
        "\n",
        "            generated_images = progan.test(latent_vectors)\n",
        "            generated_images = generated_images.to(DEVICE)\n",
        "            generated_images_resized = F.interpolate(\n",
        "                generated_images, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False\n",
        "            )\n",
        "\n",
        "            real_scores = discriminator(images)\n",
        "            fake_scores = discriminator(generated_images_resized.detach())\n",
        "            gp = gradient_penalty(discriminator, images, generated_images_resized)\n",
        "            loss_discriminator = -(real_scores.mean() - fake_scores.mean()) + 10 * gp\n",
        "\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            loss_discriminator.backward()\n",
        "            discriminator_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer.zero_grad()\n",
        "        latent_vectors = mapping_network(clip_features).to(DEVICE)\n",
        "        generated_images = progan.test(latent_vectors).to(DEVICE)\n",
        "        generated_images_resized = F.interpolate(\n",
        "            generated_images, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False\n",
        "        )\n",
        "        loss_generator = -discriminator(generated_images_resized).mean()\n",
        "\n",
        "        noise = torch.randn_like(clip_features).to(DEVICE)\n",
        "        perturbed_features = clip_features + noise\n",
        "        perturbed_latents = mapping_network(perturbed_features)\n",
        "        diverse_images = progan.test(perturbed_latents).to(DEVICE)\n",
        "        loss_diversity = diversity_loss(\n",
        "            clip_features, perturbed_features, generated_images, diverse_images\n",
        "        )\n",
        "\n",
        "        loss_reconstruction = criterion_reconstruction(\n",
        "            F.interpolate(generated_images, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False),\n",
        "            images\n",
        "        )\n",
        "        loss_identity = 1 - criterion_identity(\n",
        "            clip_features,\n",
        "            clip_model.encode_image(generated_images_resized).float()\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            features_real = perceptual_model(images)\n",
        "            features_fake = perceptual_model(F.interpolate(generated_images, size=(IMAGE_SIZE, IMAGE_SIZE), mode='bilinear', align_corners=False))\n",
        "        loss_perceptual = torch.mean((features_real - features_fake) ** 2)\n",
        "\n",
        "        total_loss = (\n",
        "            loss_reconstruction +\n",
        "            0.1 * loss_generator +\n",
        "            0.1 * loss_identity +\n",
        "            0.1 * loss_diversity +\n",
        "            0.1*loss_perceptual\n",
        "        ).mean()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, D Loss: {loss_discriminator.item():.4f}, G Loss: {loss_generator.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "# Save models\n",
        "torch.save(mapping_network.state_dict(), \"mapping_network_wgan_epoch1.pth\")\n",
        "torch.save(discriminator.state_dict(), \"discriminator_wgan_epoch1.pth\")"
      ],
      "metadata": {
        "id": "S-IDd4s_75Fv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780094e1-f58d-41eb-b97c-1dbbf98de1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, D Loss: -254.0592, G Loss: 6.8869, Total Loss: 6.2487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: also save the weights of discriminator as we are saving of the mapping network\n",
        "\n",
        "# Save the trained discriminator\n",
        "torch.save(discriminator.state_dict(), \"discriminator_newweights.pth\")"
      ],
      "metadata": {
        "id": "JCxOkdFd76TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import clip\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
        "clip_model.eval()\n",
        "\n",
        "# Load ProGAN from PyTorch Hub\n",
        "progan = torch.hub.load('facebookresearch/pytorch_GAN_zoo:hub', 'PGAN', model_name='celebAHQ-256', pretrained=True, useGPU=torch.cuda.is_available())\n",
        "\n",
        "# Load the trained Mapping Network\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=512, latent_dim=512):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(12):\n",
        "            layers.append(nn.Linear(input_dim, input_dim))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "        layers.append(nn.Linear(input_dim, latent_dim))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "mapping_network = MappingNetwork().to(DEVICE)\n",
        "# mapping_network.load_state_dict(torch.load(\"/content/mapping_network_wgan_epoch1.pth\", map_location=DEVICE))\n",
        "mapping_network.eval()\n",
        "\n",
        "# Function for Generating Images from Text\n",
        "def generate_image_from_text(text, output_path=\"generate_image.png\"):\n",
        "    # Encode text using CLIP\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(clip.tokenize([text]).to(DEVICE)).float()\n",
        "\n",
        "    # Map text features to ProGAN latent space\n",
        "    with torch.no_grad():\n",
        "        latent_vector = mapping_network(text_features)\n",
        "\n",
        "    # Generate image using ProGAN\n",
        "    with torch.no_grad():\n",
        "        generated_image = progan.test(latent_vector)\n",
        "\n",
        "    # Convert image to PIL format and save\n",
        "    generated_image = (generated_image[0].clamp(-1, 1) + 1) / 2  # Normalize to [0, 1]\n",
        "    generated_image = (generated_image * 255).permute(1, 2, 0).cpu().numpy().astype('uint8')\n",
        "    Image.fromarray(generated_image).save(output_path)\n",
        "    print(f\"Image saved at {output_path}\")\n",
        "\n",
        "# Example Usage\n",
        "text_description = \"black man\"\n",
        "generate_image_from_text(text_description)\n"
      ],
      "metadata": {
        "id": "MvBGE2lWXgjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2751d69d-fd87-4099-fb8c-a5f6b739ec00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorch_GAN_zoo_hub\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average network found !\n",
            "Image saved at generate_image.png\n"
          ]
        }
      ]
    }
  ]
}